{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333491b1-875c-4824-8e4e-192572cbe423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.4.6/libexec/bin/python: No module named python\n",
      "‚úÖ Setup listo.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, pathlib\n",
    "\n",
    "# instala paquetes en el MISMO kernel del notebook (si ya los tienes, no pasa nada)\n",
    "!{sys.executable} -m pip install -q spacy sentence-transformers scikit-learn unidecode pandas matplotlib\n",
    "!{sys.executable} -m python -m spacy download es_core_news_md\n",
    "\n",
    "# crea carpeta de salida\n",
    "pathlib.Path(\"outputs\").mkdir(exist_ok=True)\n",
    "print(\"‚úÖ Setup listo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea20601-dad4-4d39-ac37-7c560c49d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== PAR√ÅMETROS ==================\n",
    "FILE_PATH = \"../data/martes-interdisciplina-estudiantes-josefa-quezada.txt\"  # üëà cambia aqu√≠\n",
    "AXIS_TERMS = [\n",
    "    \"interdisciplina\",\"interdisciplinar\",\"interdisciplinario\",\n",
    "    \"trabajo interdisciplinario\",\"cruce de disciplinas\",\"colaboraci√≥n entre disciplinas\"\n",
    "]\n",
    "\n",
    "# Opciones r√°pidas (puedes dejarlas igual y solo cambiar FILE_PATH)\n",
    "TOP_SENTENCES = 120           # oraciones candidatas antes de recortar a fragmentos\n",
    "SIM_THRESHOLD_PERCENTILE = 55 # laxo para no perder material\n",
    "MIN_SNIPPET_WORDS = 2\n",
    "MAX_SNIPPET_WORDS = 8\n",
    "MIN_CONCEPT_WORDS = 2\n",
    "MAX_CONCEPT_WORDS = 3\n",
    "\n",
    "DESIGN_LEXICON = {\n",
    "    \"dise√±o\",\"proyecto\",\"prototipo\",\"investigaci√≥n\",\"experiencia\",\"narrativa\",\"interfaz\",\n",
    "    \"materialidad\",\"tipograf√≠a\",\"espacio\",\"interacci√≥n\",\"usuario\",\"visual\",\"sonoro\",\n",
    "    \"exhibici√≥n\",\"instalaci√≥n\",\"especulativo\",\"metodolog√≠a\",\"iteraci√≥n\",\"proceso\",\n",
    "    \"tecnolog√≠a\",\"multimedia\",\"analogico\",\"digital\",\"sensorial\",\"arquitect√≥nico\",\"inmersivo\",\n",
    "    \"co-creaci√≥n\",\"cocreaci√≥n\",\"codise√±o\",\"co-dise√±o\",\"usabilidad\",\"prototipado\"\n",
    "}\n",
    "STOP_SNIPPETS = {\n",
    "    \"gracias\",\"muchas gracias\",\"ok\",\"vale\",\"s√≠\",\"si\",\"hola\",\"buenas\",\"buenos dias\",\"buenas tardes\",\"buenas noches\",\n",
    "    \"este video\",\"el video\",\"video\"\n",
    "}\n",
    "\n",
    "# ================== C√ìDIGO BASE ==================\n",
    "import re, json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_md\")\n",
    "except Exception:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Embeddings (opcional)\n",
    "use_st = True\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "except Exception:\n",
    "    print(\"‚ö†Ô∏è Sin Sentence-Transformers: usar√© similitud de spaCy como respaldo.\")\n",
    "    use_st = False\n",
    "\n",
    "# -------- utilidades --------\n",
    "def _normalize_token(t):\n",
    "    from unidecode import unidecode\n",
    "    t = re.sub(r\"[^\\w√°√©√≠√≥√∫√±√º√Å√â√ç√ì√ö√ë√ú-]+\", \"\", t.lower())\n",
    "    return unidecode(t)\n",
    "\n",
    "def _contains_anchor(phrase_norm, axis_terms, lexicon):\n",
    "    toks = phrase_norm.split()\n",
    "    return any(\n",
    "        (tok in lexicon) or re.search(r\"^interdisciplin\", tok) for tok in toks\n",
    "    )\n",
    "\n",
    "def _trim_phrase(tokens, min_w, max_w, axis_terms, lexicon, stop_set):\n",
    "    toks = [_normalize_token(t) for t in tokens if _normalize_token(t)]\n",
    "    if len(toks) < min_w:\n",
    "        return \"\"\n",
    "    if len(toks) > max_w:\n",
    "        toks = toks[:max_w]\n",
    "    phrase = \" \".join(toks)\n",
    "    if phrase in stop_set:\n",
    "        return \"\"\n",
    "    if len(set(toks)) == 1 and len(toks) <= 3:\n",
    "        return \"\"\n",
    "    if not _contains_anchor(phrase, axis_terms, lexicon):\n",
    "        return \"\"\n",
    "    return phrase\n",
    "\n",
    "def _extract_snippets(sentences, axis_terms, design_lexicon,\n",
    "                      top_sents=120, sim_pct=55,\n",
    "                      min_w=2, max_w=8,\n",
    "                      stop_snippets=STOP_SNIPPETS):\n",
    "    # similitud al eje\n",
    "    axis_regex = re.compile(r\"\\binterdisciplin\\w*|interdisciplina\\w*\\b\", flags=re.IGNORECASE)\n",
    "    if use_st:\n",
    "        sent_emb = embedder.encode(sentences, convert_to_tensor=True, normalize_embeddings=True)\n",
    "        axis_emb = embedder.encode(axis_terms, convert_to_tensor=True, normalize_embeddings=True)\n",
    "        axis_vec = axis_emb.mean(dim=0)\n",
    "        sims = util.cos_sim(sent_emb, axis_vec).cpu().numpy().reshape(-1)\n",
    "    else:\n",
    "        axis_doc = nlp(\" \".join(axis_terms))\n",
    "        sims = np.array([nlp(s).similarity(axis_doc) for s in sentences], dtype=\"float32\")\n",
    "\n",
    "    threshold = np.percentile(sims, sim_pct)\n",
    "    idx_sorted = np.argsort(-sims)\n",
    "    sem_idx = [i for i in idx_sorted if sims[i] >= threshold][:top_sents]\n",
    "    kw_idx = [i for i, s in enumerate(sentences) if axis_regex.search(s)]\n",
    "\n",
    "    candidate_idx, seen = [], set()\n",
    "    for i in idx_sorted:\n",
    "        if i in sem_idx or i in kw_idx:\n",
    "            if i not in seen:\n",
    "                candidate_idx.append(i); seen.add(i)\n",
    "        if len(candidate_idx) >= top_sents:\n",
    "            break\n",
    "    candidate_sents = [sentences[i] for i in candidate_idx]\n",
    "\n",
    "    snippets, seen_snips = [], set()\n",
    "    for s in candidate_sents:\n",
    "        d = nlp(s)\n",
    "        toks = s.split()\n",
    "\n",
    "        # ventanas alrededor de la keyword\n",
    "        kw_positions = [i for i, t in enumerate(toks) if axis_regex.search(t)]\n",
    "        for pos in kw_positions:\n",
    "            radius = max(1, (max_w - 1)//2)\n",
    "            left = max(0, pos - radius)\n",
    "            right = min(len(toks), pos + radius + 1)\n",
    "            phrase = _trim_phrase(toks[left:right], min_w, max_w, axis_terms, design_lexicon, stop_snippets)\n",
    "            if phrase and phrase not in seen_snips:\n",
    "                snippets.append(phrase); seen_snips.add(phrase)\n",
    "\n",
    "        # noun chunks\n",
    "        for nc in d.noun_chunks:\n",
    "            phrase = _trim_phrase([t.text for t in nc], min_w, max_w, axis_terms, design_lexicon, stop_snippets)\n",
    "            if phrase and phrase not in seen_snips:\n",
    "                snippets.append(phrase); seen_snips.add(phrase)\n",
    "\n",
    "        # secuencias ADJ/NOUN/PROPN\n",
    "        buf = []\n",
    "        for t in d:\n",
    "            if t.pos_ in (\"ADJ\",\"NOUN\",\"PROPN\") and not t.is_stop:\n",
    "                buf.append(t.text)\n",
    "            else:\n",
    "                phrase = _trim_phrase(buf, min_w, max_w, axis_terms, design_lexicon, stop_snippets)\n",
    "                if phrase and phrase not in seen_snips:\n",
    "                    snippets.append(phrase); seen_snips.add(phrase)\n",
    "                buf = []\n",
    "        phrase = _trim_phrase(buf, min_w, max_w, axis_terms, design_lexicon, stop_snippets)\n",
    "        if phrase and phrase not in seen_snips:\n",
    "            snippets.append(phrase); seen_snips.add(phrase)\n",
    "\n",
    "    # refuerzo si quedaron pocos\n",
    "    if len(snippets) < 20:\n",
    "        for s in candidate_sents[:60]:\n",
    "            toks = s.split()\n",
    "            phrase = _trim_phrase(toks[:max_w+2], min_w, max_w, axis_terms, design_lexicon, stop_snippets)\n",
    "            if phrase and phrase not in seen_snips:\n",
    "                snippets.append(phrase); seen_snips.add(phrase)\n",
    "\n",
    "    return snippets, (axis_vec if use_st else None)\n",
    "\n",
    "def _extract_concepts(snips, min_w=2, max_w=3, axis_terms=None, design_lexicon=None):\n",
    "    concepts = []\n",
    "    for sn in snips:\n",
    "        d = nlp(sn)\n",
    "        buf = []\n",
    "        for t in d:\n",
    "            if t.pos_ in (\"ADJ\",\"NOUN\",\"PROPN\") and not t.is_stop and len(t.lemma_)>1:\n",
    "                buf.append(_normalize_token(t.lemma_.lower()))\n",
    "        if len(buf) >= min_w:\n",
    "            buf = buf[:max_w]\n",
    "            c = \" \".join(buf)\n",
    "            # exige al menos una ancla del l√©xico/eje\n",
    "            if _contains_anchor(c, axis_terms or [], design_lexicon or set()):\n",
    "                concepts.append(c)\n",
    "    return Counter(concepts)\n",
    "\n",
    "def run_analysis(file_path, axis_terms=AXIS_TERMS,\n",
    "                 top_sentences=TOP_SENTENCES, sim_pct=SIM_THRESHOLD_PERCENTILE,\n",
    "                 min_snip=MIN_SNIPPET_WORDS, max_snip=MAX_SNIPPET_WORDS,\n",
    "                 min_conc=MIN_CONCEPT_WORDS, max_conc=MAX_CONCEPT_WORDS,\n",
    "                 design_lexicon=DESIGN_LEXICON):\n",
    "    text = Path(file_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    doc = nlp(text)\n",
    "    sentences = [s.text.strip() for s in doc.sents if s.text.strip()]\n",
    "    if not sentences:\n",
    "        raise ValueError(\"No se detectaron oraciones.\")\n",
    "\n",
    "    # --- snippets & conceptos ---\n",
    "    snippets, axis_vec = _extract_snippets(\n",
    "        sentences, axis_terms, design_lexicon,\n",
    "        top_sents=top_sentences, sim_pct=sim_pct,\n",
    "        min_w=min_snip, max_w=max_snip, stop_snippets=STOP_SNIPPETS\n",
    "    )\n",
    "    concept_freq = _extract_concepts(snippets, min_w=min_conc, max_w=max_conc,\n",
    "                                     axis_terms=axis_terms, design_lexicon=design_lexicon)\n",
    "\n",
    "    # --- clusters opcionales ---\n",
    "    clusters_out = []\n",
    "    if use_st and snippets:\n",
    "        from sklearn.cluster import KMeans\n",
    "        X = embedder.encode(snippets, normalize_embeddings=True)\n",
    "        k = 2 if len(snippets) < 30 else (3 if len(snippets) < 60 else 4)\n",
    "        kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        buckets = defaultdict(list)\n",
    "        for lab, sn in zip(labels, snippets):\n",
    "            buckets[int(lab)].append(sn)\n",
    "        for c, items in buckets.items():\n",
    "            sims_sn = util.cos_sim(embedder.encode(items, convert_to_tensor=True, normalize_embeddings=True),\n",
    "                                   axis_vec).cpu().numpy().reshape(-1) if axis_vec is not None else np.zeros(len(items))\n",
    "            rep = items[int(np.argmax(sims_sn))] if len(items)>0 else items[0]\n",
    "            clusters_out.append({\n",
    "                \"cluster\": int(c),\n",
    "                \"n_snippets\": len(items),\n",
    "                \"representative\": rep,\n",
    "                \"snippets\": items[:10]\n",
    "            })\n",
    "\n",
    "    # --- rutas de salida organizadas ---\n",
    "    stem = Path(file_path).stem\n",
    "    out_dir = Path(\"resumen de analisis\") / stem\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    (out_dir / f\"{stem}_snippets.csv\").write_text(\n",
    "        pd.DataFrame({\"snippet\": snippets}).to_csv(index=False), encoding=\"utf-8\"\n",
    "    )\n",
    "    (out_dir / f\"{stem}_conceptos.csv\").write_text(\n",
    "        pd.DataFrame([{\"concept\": w, \"freq\": f} for w, f in concept_freq.most_common(50)]).to_csv(index=False),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    summary = {\n",
    "        \"axis_terms\": axis_terms,\n",
    "        \"file\": str(file_path),\n",
    "        \"snippets\": snippets[:120],\n",
    "        \"concepts\": [{\"concept\": w, \"freq\": int(f)} for w, f in concept_freq.most_common(50)],\n",
    "        \"clusters\": clusters_out\n",
    "    }\n",
    "    (out_dir / f\"{stem}_resumen.json\").write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # salida legible\n",
    "    print(f\"=== ARCHIVO: {stem} ===\")\n",
    "    print(f\"Resultados guardados en: {out_dir}\")\n",
    "    print(\"\\n‚Äî Fragmentos (muestra) ‚Äî\")\n",
    "    for s in snippets[:20]: print(\"-\", s)\n",
    "    print(\"\\n‚Äî Conceptos (top 15) ‚Äî\")\n",
    "    for w, f in concept_freq.most_common(15): print(f\"- {w} ({f})\")\n",
    "    if clusters_out:\n",
    "        print(\"\\n‚Äî Subtemas (clusters) ‚Äî\")\n",
    "        for cl in sorted(clusters_out, key=lambda d: d[\"cluster\"]):\n",
    "            print(f\"\\n[Cluster {cl['cluster']}] n={cl['n_snippets']}\")\n",
    "            print(\"Representativa:\", cl[\"representative\"])\n",
    "            for s in cl[\"snippets\"]: print(\"-\", s)\n",
    "\n",
    "    # exportar\n",
    "    stem = Path(file_path).stem\n",
    "    out_prefix = Path(\"outputs\") / stem\n",
    "    pd.DataFrame({\"snippet\": snippets}).to_csv(f\"{out_prefix}_snippets.csv\", index=False)\n",
    "    pd.DataFrame([{\"concept\": w, \"freq\": f} for w, f in concept_freq.most_common(50)]).to_csv(f\"{out_prefix}_conceptos.csv\", index=False)\n",
    "\n",
    "    summary = {\n",
    "        \"axis_terms\": axis_terms,\n",
    "        \"file\": str(file_path),\n",
    "        \"snippets\": snippets[:120],\n",
    "        \"concepts\": [{\"concept\": w, \"freq\": int(f)} for w, f in concept_freq.most_common(50)],\n",
    "        \"clusters\": clusters_out\n",
    "    }\n",
    "    Path(f\"{out_prefix}_resumen.json\").write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # salida legible\n",
    "    print(f\"=== ARCHIVO: {stem} ===\")\n",
    "    print(\"\\n‚Äî Fragmentos (muestra) ‚Äî\")\n",
    "    for s in snippets[:30]: print(\"-\", s)\n",
    "    print(\"\\n‚Äî Conceptos (top 20) ‚Äî\")\n",
    "    for w, f in concept_freq.most_common(20): print(f\"- {w} ({f})\")\n",
    "    if clusters_out:\n",
    "        print(\"\\n‚Äî Subtemas (clusters) ‚Äî\")\n",
    "        for cl in sorted(clusters_out, key=lambda d: d[\"cluster\"]):\n",
    "            print(f\"\\n[Cluster {cl['cluster']}] n={cl['n_snippets']}\")\n",
    "            print(\"Representativa:\", cl[\"representative\"])\n",
    "            for s in cl[\"snippets\"]: print(\"-\", s)\n",
    "    print(f\"\\n‚úÖ Exportados: {out_prefix}_snippets.csv, {out_prefix}_conceptos.csv, {out_prefix}_resumen.json\")\n",
    "\n",
    "# ---- Ejecutar an√°lisis con los par√°metros de arriba ----\n",
    "run_analysis(FILE_PATH, axis_terms=AXIS_TERMS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
